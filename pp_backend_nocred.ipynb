{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount the Google *DRIVE*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aebuD-wUNXyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxX9-MKgJEjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd4e009-0d11-4b0e-9d27-283de4838383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Mount the GDrive containing GitHub local repository to Python environment\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions -- Load JSONs on GDrive to DataFrames**"
      ],
      "metadata": {
        "id": "K6R3gSfjNlEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Functions to load data from JSON files on GDrive and map them to dataframes\n",
        "  for analysis.\n",
        "'''\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "#is_print=True\n",
        "# Get absolute paths to all files in a directory\n",
        "def get_abs_fpaths(dir):\n",
        "    for dirpath,_,filenames in os.walk(dir):\n",
        "        for f in filenames:\n",
        "            yield os.path.abspath(os.path.join(dirpath, f))\n",
        "\n",
        "# Load a JSON data file into a dataframe,given the full path\n",
        "def load_file (f):\n",
        "  try:\n",
        "    df = pd.read_json(f)\n",
        "    df.drop(columns=['success','code'])\n",
        "    return df\n",
        "  except:\n",
        "    raise Exception(f'''File not found: {f}''')\n",
        "\n",
        "# Decode common record attributes from a JSON file\n",
        "def load_common(f, report_type, retd):\n",
        "  #print('''decode_common() - Start ''')\n",
        "  state=None\n",
        "  year=None\n",
        "  quarter=None\n",
        "\n",
        "  if 'state' in f:\n",
        "    state,year,quarter = os.path.normpath(f).split(os.sep)[-3:]\n",
        "  else:\n",
        "    year,quarter=os.path.normpath(f).split(os.sep)[-2:]\n",
        "  quarter=quarter.split('.')[0]\n",
        "  geo_type='CON'\n",
        "  geo_name='india'\n",
        "  geo_parent=None\n",
        "  if year is not None and quarter is not None:\n",
        "    if state is not None:\n",
        "      geo_type='STA'\n",
        "      geo_name=state\n",
        "      geo_parent='india'\n",
        "\n",
        "  #print('''decode_common() - End ''')\n",
        "  retd['year'].append(year)\n",
        "  retd['quarter'].append(quarter)\n",
        "  retd['geo_type'].append(geo_type)\n",
        "  retd['geo_name'].append(geo_name)\n",
        "  retd['geo_parent'].append(geo_parent)\n",
        "\n",
        "  if report_type == 'top':\n",
        "    retd['top_in'].append(geo_type)\n",
        "\n",
        "  return\n",
        "\n",
        "def load_top_txns(f,report_type,retd):\n",
        "  #print('load_top_txns() - Start')\n",
        "  # Load JSON contents into a temp dataframe\n",
        "  df=load_file(f)\n",
        "\n",
        "  # Seek to the contents of the 'transaction' data\n",
        "  txn_states = df.loc['states','data']\n",
        "  txn_districts = df.loc['districts','data']\n",
        "  txn_pincodes = df.loc['pincodes','data']\n",
        "  if txn_states is not None:\n",
        "    for txn in txn_states:\n",
        "      load_common(f, report_type, retd)\n",
        "      #retd['category'].append(None)\n",
        "      retd['geo_parent'][-1]='india'\n",
        "      retd['geo_type'][-1]='STA'\n",
        "      retd['geo_name'][-1]=txn['entityName']\n",
        "      retd['stat_type'].append(txn['metric'][\"type\"])\n",
        "      retd['count'].append(txn['metric'][\"count\"])\n",
        "      retd['amount'].append(txn['metric'][\"amount\"])\n",
        "\n",
        "  if txn_districts is not None:\n",
        "    for txn in txn_districts:\n",
        "      load_common(f, report_type, retd)\n",
        "      #retd['category'].append(None)\n",
        "      retd['geo_parent'][-1]=retd['geo_name'][-1]\n",
        "      retd['geo_type'][-1]='DIS'\n",
        "      retd['geo_name'][-1]=txn['entityName']\n",
        "      retd['stat_type'].append(txn['metric'][\"type\"])\n",
        "      retd['count'].append(txn['metric'][\"count\"])\n",
        "      retd['amount'].append(txn['metric'][\"amount\"])\n",
        "\n",
        "  if txn_pincodes is not None:\n",
        "    for txn in txn_pincodes:\n",
        "      load_common(f, report_type, retd)\n",
        "      #retd['category'].append(None)\n",
        "      retd['geo_parent'][-1]=retd['geo_name'][-1]\n",
        "      retd['geo_type'][-1]='PIN'\n",
        "      retd['geo_name'][-1]=txn['entityName']\n",
        "      retd['stat_type'].append(txn['metric'][\"type\"])\n",
        "      retd['count'].append(txn['metric'][\"count\"])\n",
        "      retd['amount'].append(txn['metric'][\"amount\"])\n",
        "\n",
        "  return\n",
        "\n",
        "def load_hover_txns(f,report_type,retd):\n",
        "  print('load_hover_txns() - Start Line 96')\n",
        "  # Load JSON contents into a temp dataframe\n",
        "  df=load_file(f)\n",
        "  # Seek to the contents of the 'hover transaction' data\n",
        "  hover_recs = df.loc['hoverDataList','data']\n",
        "  if hover_recs is not None:\n",
        "    for hover_rec in hover_recs:\n",
        "      load_common(f, report_type, retd)\n",
        "      print(hover_rec)\n",
        "      retd['category'].append(hover_rec[\"name\"])\n",
        "      retd['stat_type'].append(hover_rec['metric'][\"type\"])\n",
        "      retd['count'].append(hover_rec['metric'][\"count\"])\n",
        "      retd['amount'].append(hover_rec['metric'][\"amount\"])\n",
        "\n",
        "  return None\n",
        "#  Decode 'transaction' record\n",
        "\n",
        "def load_agg_txns(f,report_type,retd):\n",
        "  #print('load_agg_txns() - Start')\n",
        "\n",
        "  # Load JSON contents into a temp dataframe\n",
        "  df=load_file(f)\n",
        "\n",
        "  # Seek to the contents of the 'transaction' data\n",
        "  txn_recs = df.loc['transactionData','data']\n",
        "  for txn_rec in txn_recs:\n",
        "    # Collect the common fields from path name\n",
        "    load_common(f, report_type, retd)\n",
        "\n",
        "    # Process record for all category\n",
        "    for payment_rec in txn_rec['paymentInstruments']:\n",
        "      retd['category'].append(txn_rec[\"name\"])\n",
        "      retd['stat_type'].append(payment_rec[\"type\"])\n",
        "      retd['count'].append(payment_rec[\"count\"])\n",
        "      retd['amount'].append(payment_rec[\"amount\"])\n",
        "\n",
        "  #print('decode_txn() - End')\n",
        "  return\n",
        "\n",
        "def load_top_users(f,report_type,retd):\n",
        "  # Load JSON contents into a temp dataframe\n",
        "  df=load_file(f)\n",
        "\n",
        "  # Seek to the contents of the 'transaction' data\n",
        "  user_states = df.loc['states','data']\n",
        "  user_districts = df.loc['districts','data']\n",
        "  user_pincodes = df.loc['pincodes','data']\n",
        "\n",
        "  if user_states is not None:\n",
        "    for user in user_states:\n",
        "      load_common(f,report_type,retd)\n",
        "      retd['geo_parent'][-1]='india'\n",
        "      retd['geo_type'][-1]='STA'\n",
        "      retd['geo_name'][-1]=user['name']\n",
        "      retd['reg_users'].append(user['registeredUsers'])\n",
        "\n",
        "  if user_districts is not None:\n",
        "    for user in user_districts:\n",
        "      load_common(f,report_type,retd)\n",
        "      if retd['top_in'][-1] == 'STA':\n",
        "        retd['geo_parent'][-1]=retd['geo_name'][-1]\n",
        "      retd['geo_type'][-1]='DIS'\n",
        "      retd['geo_name'][-1]=user['name']\n",
        "      retd['reg_users'].append(user['registeredUsers'])\n",
        "\n",
        "  if user_pincodes is not None:\n",
        "    for user in user_pincodes:\n",
        "      load_common(f,report_type,retd)\n",
        "      if retd['top_in'][-1] == 'STA':\n",
        "        retd['geo_parent'][-1]=retd['geo_name'][-1]\n",
        "      retd['geo_type'][-1]='PIN'\n",
        "      retd['geo_name'][-1]=user['name']\n",
        "      retd['reg_users'].append(user['registeredUsers'])\n",
        "\n",
        "\n",
        "# Decode 'user' record\n",
        "def load_agg_users(f,record_class,retd):\n",
        "  #print('decode_user - Start')\n",
        "\n",
        "  # Load JSON contents into a temp dataframe\n",
        "  df=load_file(f)\n",
        "\n",
        "  # Seek to the contents of the 'user.aggregated' data\n",
        "  user_stat_rec = df.loc['aggregated','data']\n",
        "\n",
        "  #print(f)\n",
        "  #print (df.to_markdown())\n",
        "  # Seek to the contents of the 'user.device' data\n",
        "  device_recs = df.loc['usersByDevice','data']\n",
        "\n",
        "  # For each device row populate both aggregate and device fields\n",
        "  if device_recs is not None:\n",
        "    for device_rec in device_recs:\n",
        "      # Collect the common fields from path name\n",
        "      load_common(f, record_class,retd)\n",
        "      retd['reg_users'].append(user_stat_rec['registeredUsers'])\n",
        "      retd['app_opens'].append(user_stat_rec['appOpens'])\n",
        "      retd['brand'].append(device_rec['brand'])\n",
        "      retd['count'].append(device_rec['count'])\n",
        "      retd['percentage'].append(device_rec['percentage'])\n",
        "  else:\n",
        "    load_common(f, record_class,retd)\n",
        "    retd['reg_users'].append(user_stat_rec['registeredUsers'])\n",
        "    retd['app_opens'].append(user_stat_rec['appOpens'])\n",
        "    retd['brand'].append('Unknown')\n",
        "    retd['count'].append(0)\n",
        "    retd['percentage'].append(100)\n",
        "\n",
        "  #print('decode_user - End')\n",
        "  return\n",
        "\n",
        "def load_hover_users(f,record_class,retd):\n",
        "  return None\n",
        "\n",
        "def load_top_ins(f,report_type,retd):\n",
        "  # Load JSON\n",
        "  df=load_file(f)\n",
        "\n",
        "  # Seek to the contents of the 'transaction' data\n",
        "  ins_states = df.loc['states','data']\n",
        "  ins_districts = df.loc['districts','data']\n",
        "  ins_pincodes = df.loc['pincodes','data']\n",
        "\n",
        "  if ins_states is not None:\n",
        "    for ins in ins_states:\n",
        "      load_common(f,report_type,retd)\n",
        "      retd['geo_parent'][-1]='india'\n",
        "      retd['geo_type'][-1]='STA'\n",
        "      retd['geo_name'][-1]=ins['entityName']\n",
        "      retd['stat_type'].append(ins['metric'][\"type\"])\n",
        "      retd['count'].append(ins['metric'][\"count\"])\n",
        "      retd['amount'].append(ins['metric'][\"amount\"])\n",
        "\n",
        "    if ins_districts is not None:\n",
        "      for ins in ins_districts:\n",
        "        load_common(f,report_type,retd)\n",
        "        if retd['top_in'][-1] == 'STA':\n",
        "          retd['geo_parent'][-1]=retd['geo_name'][-1]\n",
        "        retd['geo_type'][-1]='DIS'\n",
        "        retd['geo_name'][-1]=ins['entityName']\n",
        "        retd['stat_type'].append(ins['metric'][\"type\"])\n",
        "        retd['count'].append(ins['metric'][\"count\"])\n",
        "        retd['amount'].append(ins['metric'][\"amount\"])\n",
        "\n",
        "    if ins_pincodes is not None:\n",
        "      for ins in ins_pincodes:\n",
        "        load_common(f,report_type, retd)\n",
        "        if retd['top_in'][-1] == 'STA':\n",
        "          retd['geo_parent'][-1]=retd['geo_name'][-1]\n",
        "        retd['geo_type'][-1]='PIN'\n",
        "        retd['geo_name'][-1]=ins['entityName']\n",
        "        retd['stat_type'].append(ins['metric'][\"type\"])\n",
        "        retd['count'].append(ins['metric'][\"count\"])\n",
        "        retd['amount'].append(ins['metric'][\"amount\"])\n",
        "\n",
        "# Decode 'insurance' record\n",
        "def load_agg_ins(f,report_type,retd):\n",
        "  #print('decode_ins - Start')\n",
        "\n",
        "  # Load JSON contents into a temp dataframe\n",
        "  df=load_file(f)\n",
        "\n",
        "  # print(df.to_markdown())\n",
        "  # Seek to the contents of the 'insurance' data\n",
        "  ins_recs = df.loc['transactionData','data']\n",
        "\n",
        "  # For each 'insurance' row populate fields\n",
        "  for ins_rec in ins_recs:\n",
        "    # Collect the common fields from path name\n",
        "    load_common(f, report_type, retd)\n",
        "    for payment_rec in ins_rec['paymentInstruments']:\n",
        "      retd['category'].append(ins_rec[\"name\"])\n",
        "      retd['stat_type'].append(payment_rec[\"type\"])\n",
        "      retd['count'].append(payment_rec[\"count\"])\n",
        "      retd['amount'].append(payment_rec[\"amount\"])\n",
        "  #print('decode_ins - End')\n",
        "  return\n",
        "\n",
        "def load_hover_ins(f,report_type,retd):\n",
        "  return None\n",
        "\n",
        "# Supported record types\n",
        "record_types = {\n",
        "  \"transaction\":{\n",
        "    \"report_type\":{\n",
        "      \"aggregated\":{'decoder':load_agg_txns,\"columns\":['year','quarter', 'geo_type', 'geo_name', 'geo_parent', 'category', 'stat_type', 'count', 'amount']},\n",
        "      \"top\":{'decoder':load_top_txns,'columns':['year','quarter', 'geo_type', 'geo_name', 'geo_parent', 'top_in', 'stat_type', 'count', 'amount']},\n",
        "      #\"map\":{'decoder':load_hover_txns,\"columns\":['year','quarter', 'geo_type', 'geo_name', 'geo_parent', 'category', 'stat_type', 'count', 'amount']}\n",
        "    }\n",
        "  },\n",
        "\n",
        "  \"user\":{\n",
        "    \"report_type\":{\n",
        "       \"aggregated\":{'decoder':load_agg_users,\"columns\":['year','quarter', 'geo_type', 'geo_name', 'geo_parent', 'reg_users', 'app_opens','brand','count','percentage'],},\n",
        "       \"top\":{'decoder':load_top_users,'columns':['year','quarter', 'geo_type', 'geo_name', 'geo_parent','top_in','reg_users']},\n",
        "       #\"map\":{'decoder':load_hover_users,\"columns\":['year','quarter', 'geo_type', 'geo_name', 'geo_parent', 'reg_users', 'app_opens','brand','count','percentage'],}\n",
        "    }\n",
        "  },\n",
        "\n",
        "  \"insurance\":{\n",
        "    \"report_type\":{\n",
        "      \"aggregated\":{'decoder':load_agg_ins,\"columns\":['year','quarter', 'geo_type', 'geo_name', 'geo_parent','category', 'stat_type', 'count', 'amount'],},\n",
        "      \"top\":{'decoder':load_top_ins,'columns':['year','quarter', 'geo_type', 'geo_name','geo_parent', 'top_in', 'stat_type', 'count', 'amount']},\n",
        "      #\"map\":{'decoder':load_hover_ins,\"columns\":['year','quarter', 'geo_type', 'geo_name','geo_parent', 'category', 'stat_type', 'count', 'amount'],}\n",
        "     }\n",
        "  }\n",
        "}\n",
        "\n",
        "# Supported record classes\n",
        "#report_type = {\n",
        "#    \"aggregated\":\"Aggregated\",\n",
        "#    \"hover\": \"Hover\",\n",
        "#    \"top\": \"Top\"\n",
        "#}\n",
        "\n",
        "def get_abs_fpaths(dir):\n",
        "  for dirpath,_,filenames in os.walk(dir):\n",
        "    for f in filenames:\n",
        "      yield os.path.abspath(os.path.join(dirpath, f))\n",
        "\n",
        "# Load data\n",
        "def load_data(root_dir):\n",
        "\n",
        "  print( '''load_data() - Start''')\n",
        "  # Dictionary of resulting dataframes\n",
        "  ret = dict()\n",
        "\n",
        "  # Get aggregate, top and map for each record type\n",
        "  for rec_type in record_types.keys(): # trans, user, insurance\n",
        "    ret[rec_type]=dict()\n",
        "    #print(f'''Loading {rec_type} data''')\n",
        "    # Do this for every report_type - aggregate, top and map/hover\n",
        "    for report_type in record_types[rec_type]['report_type'].keys():\n",
        "      # Set the columns of output dataframe\n",
        "      columns = record_types[rec_type]['report_type'][report_type]['columns']\n",
        "      #print(f'''Loading {report_type} data''')\n",
        "\n",
        "      # Create an output dataframe to store record's from multiple files\n",
        "      #ret_df = pd.DataFrame(columns)\n",
        "      retd = dict()\n",
        "      for c in columns:\n",
        "        retd[c]=list()\n",
        "\n",
        "      # Add the output dataframe to the output dictionary of dataframes\n",
        "      ret[rec_type][report_type]=retd\n",
        "\n",
        "      # Construct absolute path to folder containing files for\n",
        "      # a record class and type\n",
        "      full_path=f'''{root_dir}/{report_type}/{rec_type}'''\n",
        "      if report_type == 'map':\n",
        "        full_path += '/hover'\n",
        "      # Decode contents of each file accumulate in a dataframe\n",
        "      count=0\n",
        "      print({full_path})\n",
        "      for f in get_abs_fpaths(full_path):\n",
        "        loader = record_types[rec_type]['report_type'][report_type]['decoder']\n",
        "        loader(f,report_type,retd)\n",
        "        count +=1\n",
        "\n",
        "  print( f'''load_data() {count} files processed - End''')\n",
        "  return ret\n",
        "\n",
        "  '''\n",
        "  Helper functions to pre-process 'transaction' data for DB writes\n",
        "'''\n",
        "import sqlalchemy\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "\n",
        "def get_geo_info(connection):\n",
        "  sql = '''SELECT id,name,geo_type,parent_id FROM geo;'''\n",
        "  df=pd.read_sql_query(sql, connection)\n",
        "  return df\n",
        "\n",
        "def lookup_geo_id(geo_type, geo_name, geo_parent,geo):\n",
        "  if geo_type == 'STA':\n",
        "    geo_name=geo_name.replace(' ','-')\n",
        "  if geo_name == 'nan':\n",
        "    print(f'''geo_type = {geo_type}, geo_parent = {geo_parent}''')\n",
        "  #print(f'''geo_type:{geo_type}, geo_name:{geo_name}''')\n",
        "  geo_entries = geo[(geo['geo_type']==geo_type) & (geo['name']==geo_name)].reset_index()\n",
        "  if geo_entries.shape[0] == 1:\n",
        "    return geo_entries.loc[0,'id']\n",
        "  elif geo_entries.shape[0]>1:\n",
        "    if geo_parent is not None:\n",
        "      geo_parents = geo[geo['name']==geo_parent].reset_index()\n",
        "      if geo_parents.shape[0] > 0:\n",
        "        return geo_entries[geo_entries['parent_id']==geo_parents.loc[0,'id']].reset_index().loc[0,'id']\n",
        "  return -1\n",
        "\n",
        "def preprocess_generic(df,geo):\n",
        "  df['geo_name']=df['geo_name'].astype(str)\n",
        "\n",
        "  # Lookup the geo_id from type and name\n",
        "#  df['geo_typename']=df['geo_type'] + ',' + df['geo_name']\n",
        "#  df['geo_id']=df['geo_typename'].apply(lookup_geo_id, args=(geo,))\n",
        "  df['geo_id']=df.apply(lambda x: lookup_geo_id(x['geo_type'],x['geo_name'],x['geo_parent'],geo),axis=1)\n",
        "  # Drop unnecessary columns\n",
        "  df.drop(columns=['geo_type', 'geo_name', 'geo_parent'], inplace=True)\n",
        "  df = df.reset_index().rename(columns={'index':'id'})\n",
        "  return df\n",
        "\n",
        "def save_txns(agg_txns,top_txns,conn):\n",
        "  # Lookup Geo_id and drop geo_type and geo_name columns\n",
        "  # Step 1: Replace geo_type & geo_name with geo_id as a single column\n",
        "  # Step 2: Add id column\n",
        "  geo=get_geo_info(conn)\n",
        "\n",
        "  # Review the shape\n",
        "  print(f'''geo - {geo.shape}''')\n",
        "\n",
        "  agg_txns = preprocess_generic(agg_txns,geo)\n",
        "  top_txns = preprocess_generic(top_txns,geo)\n",
        "  print(f'''agg_txns(db): {agg_txns.shape}, top_txns(db):{top_txns.shape}''')\n",
        "  #print(f'''{top_txns[top_txns['geo_id']==-1].to_markdown()}''')\n",
        "  # Write to 'transaction_*' tables\n",
        "  agg_txns.to_sql(con=conn, name='transaction_agg',if_exists='append',index=False)\n",
        "  top_txns.to_sql(con=conn, name='transaction_top',if_exists='append',index=False)\n",
        "\n",
        "  conn.commit()\n",
        "\n",
        "def save_ins(agg_ins,top_ins,conn):\n",
        "  # Lookup Geo_id and drop geo_type and geo_name columns\n",
        "  # Step 1: Replace geo_type & geo_name with geo_id as a single column\n",
        "  # Step 2: Add id column\n",
        "  geo=get_geo_info(conn)\n",
        "\n",
        "  # Review the shape\n",
        "  print(f'''geo - {geo.shape}''')\n",
        "\n",
        "  agg_ins = preprocess_generic(agg_ins,geo)\n",
        "  top_ins = preprocess_generic(top_ins,geo)\n",
        "  print(f'''agg_ins(db): {agg_ins.shape}, top_ins(db):{top_ins.shape}''')\n",
        "\n",
        "  # Write to 'insurance*' tables\n",
        "  agg_ins.to_sql(con=conn, name='insurance_agg',if_exists='append',index=False)\n",
        "  top_ins.to_sql(con=conn, name='insurance_top',if_exists='append',index=False)\n",
        "\n",
        "  conn.commit()\n",
        "\n",
        "def save_users(agg_users,top_users,conn):\n",
        "  # Lookup Geo_id and drop geo_type and geo_name columns\n",
        "  # Step 1: Replace geo_type & geo_name with geo_id as a single column\n",
        "  # Step 2: Add id column\n",
        "  geo=get_geo_info(conn)\n",
        "\n",
        "  # Review the shape\n",
        "  print(f'''geo - {geo.shape}''')\n",
        "\n",
        "  agg_users = preprocess_generic(agg_users,geo)\n",
        "  top_users = preprocess_generic(top_users,geo)\n",
        "  print(f'''agg_users(db): {agg_users.shape}, top_users(db):{top_users.shape}''')\n",
        "\n",
        "  u_agg = agg_users[['year','quarter','geo_id','reg_users','app_opens']]\n",
        "  d_agg = agg_users[['year','quarter','geo_id','brand','count','percentage']].reset_index().rename(columns={'index':'id'})\n",
        "  u_agg.drop_duplicates(inplace=True,ignore_index=True)\n",
        "\n",
        "  u_agg = u_agg.reset_index().rename(columns={'index':'id'})\n",
        "  u_agg['stat_type']='TOTAL'\n",
        "  top_users['stat_type']='TOTAL'\n",
        "\n",
        "  # Write to 'users*' tables\n",
        "  u_agg.to_sql(con=conn, name='user_agg',if_exists='append',index=False)\n",
        "  d_agg.to_sql(con=conn, name='device_agg',if_exists='append',index=False)\n",
        "  top_users.to_sql(con=conn, name='user_top',if_exists='append',index=False)\n",
        "\n",
        "  conn.commit()\n",
        "  print(f'''user_agg(db): {u_agg.shape}, device_agg(db): {d_agg.shape}, top_users(db):{top_users.shape}''')\n",
        "\n",
        "'''\n",
        "  Helper functions to save Geo\n",
        "'''\n",
        "\n",
        "def lookup_state_id(state_name, states):\n",
        "    ret=None\n",
        "    ret = states[states['name']==state_name].reset_index()['id'][0]\n",
        "    return ret;\n",
        "\n",
        "def save_geo(states,districts,pincodes,conn):\n",
        "    '''\n",
        "      Prepare the 'states' DF for DB insertion\n",
        "    '''\n",
        "    # Replace the DF column names to match the DB column names\n",
        "    states.reset_index(inplace=True)\n",
        "    states.rename(columns={'index':'id', 'parent':'parent_id'},inplace=True)\n",
        "\n",
        "    # Add geo_type column and set the value to 'STA'\n",
        "    states['geo_type']='STA'\n",
        "\n",
        "    # Assign non-conflicting IDs for DB rows\n",
        "    states.id=states.id+1\n",
        "\n",
        "    '''\n",
        "      Prepare the 'districts' DF for DB insertion\n",
        "    '''\n",
        "    # Replace the DF column names to match the DB column names\n",
        "    districts.reset_index(inplace=True)\n",
        "    districts.rename(columns={'index':'id', 'parent':'parent_id'},inplace=True)\n",
        "\n",
        "    # Add geo_type column and set the value to 'DIS'\n",
        "    districts['geo_type']='DIS'\n",
        "    districts['name']=districts['name'].apply(lambda x: x.split('|')[0] )\n",
        "\n",
        "    # Assign non-conflicting DB ids for the districts\n",
        "    districts.id=districts.id+len(states.index)+1\n",
        "\n",
        "    '''\n",
        "      Prepare the 'pincodes' DF for DB insertion\n",
        "    '''\n",
        "    # Replace the DF column names to match the DB column names\n",
        "    pincodes.reset_index(inplace=True)\n",
        "    pincodes.rename(columns={'index': 'id', 'parent':'parent_id'},inplace=True)\n",
        "\n",
        "    # Add geo_type column and set the value to 'PIN'\n",
        "    pincodes['geo_type']='PIN'\n",
        "\n",
        "    # Assign non-conflicting DB ids for the districts\n",
        "    pincodes.id=pincodes.id+len(states.index)+ len(districts.index)+1\n",
        "\n",
        "    # Impute null values (NOTE: Analyis showed that - the only NA value corrsponded to 000000 for parent state ID=18)\n",
        "    pincodes['name']=pincodes.name.fillna('nan')\n",
        "\n",
        "    '''\n",
        "      Replace parent names with IDs assigned in previous steps\n",
        "    '''\n",
        "    states['parent_id']=0\n",
        "    districts['parent_id']=districts.apply(lambda x: lookup_state_id(x['parent_id'],states),axis=1)\n",
        "    pincodes['parent_id']=pincodes.apply(lambda x: lookup_state_id(x['parent_id'],states),axis=1)\n",
        "\n",
        "    # Write to the DB\n",
        "    country_df=pd.DataFrame({'id':0,'geo_type':'CON','name':'india','parent_id':None},index=[0])\n",
        "    country_df.to_sql(con=conn, name='geo',if_exists='append',index=False)\n",
        "    states.to_sql(con=conn, name='geo',if_exists='append',index=False)\n",
        "    districts.to_sql(con=conn, name='geo',if_exists='append',index=False)\n",
        "    pincodes.to_sql(con=conn, name='geo',if_exists='append',index=False)\n",
        "    conn.commit()\n",
        "\n",
        "'''\n",
        "  Index Geo information from data\n",
        "'''\n",
        "\n",
        "def index_geo_info(datadir):\n",
        "  ret = dict()\n",
        "  ret['states'] = dict()\n",
        "  ret['districts'] = dict()\n",
        "  ret['pincodes'] = dict()\n",
        "\n",
        "  for f in get_abs_fpaths(datadir):\n",
        "    state=None\n",
        "    if 'state' in f:\n",
        "      state= os.path.normpath(f).split(os.sep)[-3]\n",
        "      # Index the state\n",
        "      if state not in ret['states']:\n",
        "        ret['states'][state]='india'\n",
        "\n",
        "      if 'top' in f:\n",
        "        # Load JSON contents into a temp dataframe\n",
        "        df=load_file(f)\n",
        "\n",
        "        # Seek to the contents of the 'transaction' data\n",
        "        districts = df.loc['districts','data']\n",
        "        pincodes = df.loc['pincodes','data']\n",
        "\n",
        "        # Index the districts in the file\n",
        "        if districts is not None:\n",
        "          for entry in districts:\n",
        "            if 'entityName' in entry:\n",
        "              district = entry['entityName']\n",
        "            elif 'name' in entry:\n",
        "              district = entry['name']\n",
        "            if district not in ret['districts']:\n",
        "              ret['districts'][district] = state\n",
        "            else:\n",
        "              if ret['districts'][district] != state:\n",
        "                ret['districts'][district+\"|\"+state]=state\n",
        "\n",
        "        # Index the pincodes in the file\n",
        "        if pincodes is not None:\n",
        "          for entry in pincodes:\n",
        "            if 'entityName' in entry:\n",
        "              pincode = entry['entityName']\n",
        "            elif 'name' in entry:\n",
        "              pincode = entry['name']\n",
        "            if pincode not in ret['pincodes']:\n",
        "              ret['pincodes'][pincode] = state\n",
        "  return ret"
      ],
      "metadata": {
        "id": "pWQfoMdYJj7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions to Fetch DASHBOARDS**"
      ],
      "metadata": {
        "id": "1rWvF7d3N7fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Get Dashboards from mySQL db\n",
        "'''\n",
        "import sqlalchemy\n",
        "import pandas as pd\n",
        "\n",
        "def convert_to_geojson_state(name):\n",
        "\tif name is None:\n",
        "\t\treturn name\n",
        "\treturn ' '.join([x.capitalize() for x in name.split('-')])\n",
        "\n",
        "\n",
        "def get_years_list():\n",
        "  return [2018,2019,2020,2021,2022,2023,2024]\n",
        "\n",
        "def get_quarters_list():\n",
        "  return [(1,'Q1'),(2,'Q2'),(3,'Q3'),(4,'Q4')]\n",
        "\n",
        "def get_states_list(conn):\n",
        "  sql= f'''SELECT g.id as 'id', g.name as 'State' FROM geo g WHERE geo_type='STA' ORDER BY name ASC'''\n",
        "  # Get states from db\n",
        "  stmt = sqlalchemy.text(sql,)\n",
        "  df=pd.read_sql_query(stmt, conn)\n",
        "  return df\n",
        "\n",
        "\n",
        "def fetch_txn_dash(conn, year,quarter, state=None):\n",
        "  # dashboard dict for output\n",
        "  dashboard=dict()\n",
        "  dashboard['object']='Transaction'\n",
        "  dashboard['topic']='Transaction Details'\n",
        "  dashboard['description']='Transaction Details for selected timeline'\n",
        "  dashboard['grand_total']=dict()\n",
        "  grand_total=dashboard['grand_total']\n",
        "  grand_total['name']='Transaction Summary'\n",
        "  grand_total['charts']=list()\n",
        "\n",
        "  # Detailed Totals   -- Sub dict() being created\n",
        "  dashboard['det_total']=dict()\n",
        "  det_total=dashboard['det_total']\n",
        "  det_total['name']='Payment Modes'\n",
        "  det_total['charts']=list()\n",
        "\n",
        "  # SQLs and Filter Procesing\n",
        "  sql= f'''SELECT t.category as 'Payment Mode', sum(t.count) as 'Transaction Count', sum(t.amount) 'Transaction Amount' FROM transaction_agg t WHERE year=:year'''\n",
        "\n",
        "  # Extend the WHERE clause based on other fiters\n",
        "  params=dict()\n",
        "  params['year']=year\n",
        "  if quarter is not None:\n",
        "    sql += ' AND quarter=:quarter'\n",
        "    params['quarter']=quarter[0]\n",
        "\n",
        "  sql += ' GROUP BY t.category'\n",
        "\n",
        "  # Execute agg SQL to get df with 4 rows and 3 columns\n",
        "  stmt = sqlalchemy.text(sql,)\n",
        "  df=pd.read_sql_query(stmt, conn, params=params)\n",
        "\n",
        "  # Calculating Grand Totals for Amount/Count from df and store in dashboard of ['grand_total' & 'det_total']\n",
        "  grand_ct_total=dict()\n",
        "  grand_ct_total['title']='Transaction Count'\n",
        "  grand_ct_total['value']=0\n",
        "\n",
        "  grand_amt_total=dict()\n",
        "  grand_amt_total['title']='Transaction Amount (Rs)'\n",
        "  grand_amt_total['value'] = 0\n",
        "  for idx, row in df.iterrows():\n",
        "    det_ct=dict()\n",
        "    det_ct['title']=row['Payment Mode']               #  for count metric widget\n",
        "    det_ct['value']=row['Transaction Count']          #  for count metric widget\n",
        "    det_ct['metric']='Transaction Count'              #  for count metric widget\n",
        "    det_total['charts'].append(det_ct)\n",
        "    grand_ct_total['value'] += row['Transaction Count']             # grand total of count\n",
        "\n",
        "\n",
        "    det_amt=dict()\n",
        "    det_amt['title']=row['Payment Mode']\n",
        "    det_amt['value']=row['Transaction Amount']\n",
        "    det_amt['metric']='Transaction Amount'\n",
        "    det_total['charts'].append(det_amt)\n",
        "    grand_amt_total['value'] += row['Transaction Amount']\n",
        "\n",
        "  grand_total['charts'].append(grand_ct_total)\n",
        "  grand_total['charts'].append(grand_amt_total)\n",
        "\n",
        "  # Construct TOP Dashboards\n",
        "  if( quarter is not None):\n",
        "    dashboard['top']=dict()\n",
        "    sql_top=f'''SELECT g.name as 'Geo', g.geo_type as 'Geo Type', t.count as 'Txn Count' ,t.amount as 'Txn Amount (Rs)' FROM transaction_top t JOIN geo g ON (t.geo_id=g.id) WHERE year=:year AND quarter=:quarter AND top_in=:top_in'''\n",
        "    params_top=dict()\n",
        "    params_top['year']=year\n",
        "    params_top['quarter']=quarter[0]\n",
        "\n",
        "    if state is not None:\n",
        "      params_top['top_in']='STA'\n",
        "      sql_top += ' AND geo_id IN (SELECT id from geo WHERE parent_id=:parent_id)'\n",
        "      params_top['parent_id']=state\n",
        "    else:\n",
        "      params_top['top_in']='CON'\n",
        "\n",
        "    stmt = sqlalchemy.text(sql_top,)\n",
        "    df_top=pd.read_sql_query(stmt, conn, params=params_top)\n",
        "\n",
        "    # Populate 'top'\n",
        "    top=dashboard['top']\n",
        "    top['name']= 'Txns By Top Geo'\n",
        "    top['charts']=list()\n",
        "\n",
        "    top['charts'].append(dict())\n",
        "    top['charts'][0]['title']='Top States'\n",
        "    top['charts'][0]['data']=df_top[df_top['Geo Type']=='STA'].reset_index().drop(columns=['index']).sort_values('Txn Count', ascending=False)\n",
        "\n",
        "    top['charts'].append(dict())\n",
        "    top['charts'][1]['title']='Top Districts'\n",
        "    top['charts'][1]['data']=df_top[df_top['Geo Type']=='DIS'].reset_index().drop(columns=['index']).sort_values('Txn Count', ascending=False)\n",
        "\n",
        "    top['charts'].append(dict())\n",
        "    top['charts'][2]['title']='Top Pincodes'\n",
        "    top['charts'][2]['data']=df_top[df_top['Geo Type']=='PIN'].reset_index().drop(columns=['index']).sort_values('Txn Count', ascending=False)\n",
        "\n",
        "  # Map Data\n",
        "  if state is None:\n",
        "    dashboard['by_geo']=dict()\n",
        "    by_geo = dashboard['by_geo']\n",
        "    by_geo['name']='Insurance By State'\n",
        "\n",
        "    params_states=dict()\n",
        "    params_states['year']=year\n",
        "    sql_states = f'''SELECT S1.geo_id AS 'State Code', g.name AS 'State', S1.counts AS 'Txn Count', S1.amount AS 'Txn Amount' FROM (SELECT geo_id, sum(t.count) as counts, sum(t.amount) as amount FROM transaction_agg t WHERE year=:year'''\n",
        "    if quarter is not None:\n",
        "      sql_states += ' AND quarter=:quarter'\n",
        "      params_states['quarter']=quarter[0]\n",
        "\n",
        "    sql_states += ''' GROUP BY geo_id)S1 JOIN geo g ON (S1.geo_id=g.id) WHERE geo_type='STA' '''\n",
        "\n",
        "    stmt = sqlalchemy.text(sql_states,)\n",
        "    df_states=pd.read_sql_query(stmt, conn,params=params_states)\n",
        "    df_states['State']=df_states['State'].apply(convert_to_geojson_state)\n",
        "    by_geo['charts']=list()\n",
        "    by_geo['charts'].append({'title':'Transactions By State','data':df_states})\n",
        "  return dashboard\n",
        "'''  {\n",
        "      'recordType': 'Transaction',\n",
        "      'topic': 'Transaction Details',\n",
        "      'description':'Details for selected timeline',\n",
        "      'grand_total':{'name':'Transaction Summary','charts':[{'title': 'Transaction Count', 'value': 50000},{'title': 'Amount (in Rs.)', 'value': 78500}]},\n",
        "      'top':{'name':'Transactions By Top Locations','charts':[ {'title': 'Top States', 'data': pd.DataFrame({'State':['tamil-nadu','maharastra', 'kerala','karnataka', 'bihar','manipur','andhra-preadesh','jharkand','jammu-&-kashmir', 'puduchery'], 'Transaction Count':[10,9,8,7,6,5,4,3,2,1], 'Amount':[10000,9000,8000,7000,6000,5000,4000,3000,2000,1000]})},\n",
        "              {'title': 'Top Districts', 'data': pd.DataFrame({'District':['chengalpet','thiruvallur', 'krishnagiri','ramanathapuram','salem','namakkal','karur','coimbatore','nilgiris','hosur'], 'Transaction Count':[10,9,8,7,6,5,4,3,2,1],'Amount':[10000,9000,8000,7000,6000,5000,4000,3000,2000,1000]})},\n",
        "              {'title': 'Top Pincodes', 'data': pd.DataFrame({'Pincode':['600023','600001', '600011','600028','600038','600048','600058','600068','600078','600088'], 'Transaction Count':[10,9,8,7,6,5,4,3,2,1], 'Amount':[10000,9000,8000,7000,6000,5000,4000,3000,2000,1000]})}\n",
        "      ]},\n",
        "      'trend':{'name':'Quarterly Transaction Trends', 'charts':[{'title': 'Quarterly Transaction Trend', 'data': pd.DataFrame({'Quarter':['Q1','Q2','Q3','Q4'], 'Transaction Count':[1,4,3,2]})}]},\n",
        "      'det_total':{'name': 'Payment Modes','charts':[{'title':'Recharge & bill payments', 'value': 50000,'metric':'Count'},{'title': 'Peer-to-peer payments', 'value': 78500, 'metric':'Count'}, {'title': 'Merchant payments', 'value': 98500,'metric':'Count'},{'title': 'Financial Services', 'value': 50255,'metric':'Count'},{'title':'Recharge & bill payments', 'value': 50000,'metric':'Amount'},{'title': 'Peer-to-peer payments', 'value': 78500, 'metric':'Amount'}, {'title': 'Merchant payments', 'value': 98500,'metric':'Amount'},{'title': 'Financial Services', 'value': 50255,'metric':'Amount'}]}\n",
        "      }\n",
        "'''\n",
        "\n",
        "def fetch_ins_dash(conn, year,quarter, state=None):\n",
        "  # Construct the dashboard dictionary\n",
        "  dashboard=dict()\n",
        "  dashboard['object']='Insurance Details'\n",
        "  dashboard['title']='Insurance Details'\n",
        "  dashboard['description']='Insurance Details for Selected Timeline'\n",
        "  dashboard['grand_total']=dict()\n",
        "  grand_total=dashboard['grand_total']\n",
        "  grand_total['charts']=list()\n",
        "\n",
        "  # SQLs and Filter Procesing\n",
        "  sql= f'''SELECT i.count as 'Insurance Count', i.amount as 'Insurance Amount' FROM insurance_agg i WHERE year=:year'''\n",
        "\n",
        "  params=dict()\n",
        "  params['year']=year\n",
        "  # if quarter is specified add to WHERE clause\n",
        "  if quarter is not None:\n",
        "    sql += ' AND quarter=:quarter'\n",
        "    params['quarter']=quarter[0]\n",
        "\n",
        "  # setting the geo_id filter\n",
        "  sql += ' AND geo_id=:geo_id'\n",
        "  if state is not None:\n",
        "    params['geo_id']=state\n",
        "  else:\n",
        "    params['geo_id']=0 #Refers to country India\n",
        "\n",
        "  # Execute agg SQL\n",
        "  stmt = sqlalchemy.text(sql,)\n",
        "  df=pd.read_sql_query(stmt, conn, params=params)\n",
        "\n",
        "  # Populate 'aggregates' & 'aux_aggregates'\n",
        "  grand_ct_total=dict()\n",
        "  grand_ct_total['title']='Insurance Count'\n",
        "  grand_ct_total['value']=0\n",
        "\n",
        "  grand_amt_total=dict()\n",
        "  grand_amt_total['title']='Insurance Amount (Rs)'\n",
        "  grand_amt_total['value']=0\n",
        "\n",
        "  for idx, row in df.iterrows():\n",
        "    grand_ct_total['value'] += row['Insurance Count']\n",
        "    grand_amt_total['value'] += row['Insurance Amount']\n",
        "\n",
        "  grand_total['charts'].append(grand_ct_total)\n",
        "  grand_total['charts'].append(grand_amt_total)\n",
        "\n",
        "  # Execute Top SQL\n",
        "  if quarter is not None:\n",
        "    dashboard['top']=dict()\n",
        "    sql_top=f'''SELECT g.name as 'Geo', g.geo_type as 'Geo Type', i.count as 'Insurance Count', i.amount as 'Insurance Amount (Rs)' FROM insurance_top i JOIN geo g ON (i.geo_id=g.id) WHERE year=:year AND quarter=:quarter AND top_in=:top_in'''\n",
        "\n",
        "    params_top=dict()\n",
        "    params_top['year']=year\n",
        "    params_top['quarter']=quarter[0]\n",
        "\n",
        "    if state is not None:\n",
        "      sql_top += ' AND geo_id IN (SELECT id FROM geo WHERE parent_id=:parent_id)'\n",
        "      params_top['top_in']='STA'\n",
        "      params_top['parent_id']=state\n",
        "    else:\n",
        "      params_top['top_in']='CON'\n",
        "\n",
        "    stmt = sqlalchemy.text(sql_top,)\n",
        "    df_top=pd.read_sql_query(stmt, conn,params=params_top)\n",
        "\n",
        "    # Populate 'top'\n",
        "    top=dashboard['top']\n",
        "    top['name']= 'Insurance By Top Locations'\n",
        "    top['charts']=list()\n",
        "\n",
        "    #if state is None:\n",
        "    top['charts'].append(dict())\n",
        "    top['charts'][0]['title']='Top States'\n",
        "    top['charts'][0]['data']=df_top[df_top['Geo Type']=='STA'].reset_index().drop(columns=['index']).sort_values('Insurance Count', ascending=False)\n",
        "\n",
        "    top['charts'].append(dict())\n",
        "    #cidx= len(top['charts'])-1\n",
        "    top['charts'][1]['title']='Top Districts'\n",
        "    top['charts'][1]['data']=df_top[df_top['Geo Type']=='DIS'].reset_index().drop(columns=['index']).sort_values('Insurance Count', ascending=False)\n",
        "\n",
        "    top['charts'].append(dict())\n",
        "    #cidx= len(top['charts'])-1\n",
        "    top['charts'][2]['title']='Top Pincodes'\n",
        "    top['charts'][2]['data']=df_top[df_top['Geo Type']=='PIN'].reset_index().drop(columns=['index']).sort_values('Insurance Count', ascending=False)\n",
        "\n",
        "  # Map Data\n",
        "  if state is None:\n",
        "    dashboard['by_geo']=dict()\n",
        "    by_geo = dashboard['by_geo']\n",
        "    by_geo['name']='Insurance By State'\n",
        "\n",
        "    params_states=dict()\n",
        "    params_states['year']=year\n",
        "    sql_states = f'''SELECT S1.geo_id AS 'State Code', g.name AS 'State', S1.counts AS 'Insurance Count', S1.amount AS 'Insurance Amount' FROM (SELECT geo_id, sum(i.count) as counts, sum(i.amount) as amount FROM insurance_agg i WHERE year=:year'''\n",
        "    if quarter is not None:\n",
        "      sql_states += ' AND quarter=:quarter'\n",
        "      params_states['quarter']=quarter[0]\n",
        "\n",
        "    sql_states += ''' GROUP BY geo_id)S1 JOIN geo g ON (S1.geo_id=g.id) WHERE geo_type='STA' '''\n",
        "\n",
        "    stmt = sqlalchemy.text(sql_states,)\n",
        "    df_states=pd.read_sql_query(stmt, conn, params=params_states)\n",
        "    df_states['State']=df_states['State'].apply(convert_to_geojson_state)\n",
        "\n",
        "    by_geo['charts']=list()\n",
        "    by_geo['charts'].append({'title':'Insurance By State','data':df_states})\n",
        "  return dashboard\n",
        "\n",
        "def fetch_user_dash(conn, year,quarter, state=None):\n",
        "  dashboard=dict()\n",
        "  dashboard['object']='User'\n",
        "  dashboard['title']='User Dashboard'\n",
        "  dashboard['description']='Details of User and Devices for the selected timeline'\n",
        "\n",
        "  # Process User and Device Totals\n",
        "  dashboard['grand_total']=dict()\n",
        "  grand=dashboard['grand_total']\n",
        "  grand['name']='Registered Users'\n",
        "  grand['charts']=list()\n",
        "\n",
        "  dashboard['det_total']=dict()\n",
        "  det=dashboard['det_total']\n",
        "  det['name']='Registered Devices'\n",
        "  det['charts']=list()\n",
        "\n",
        "  # SQL Procesing\n",
        "  sql= f'''SELECT u.reg_users as 'Users', u.app_opens as 'Application Opens' FROM user_agg u WHERE year=:year'''\n",
        "  sql_device= f'''SELECT d.brand as 'Device Brand', sum(d.count) as 'Device Count', sum(d.percentage) as 'Device Share (%)' FROM device_agg d WHERE year=:year'''\n",
        "\n",
        "  # Set the WHERE clause based on the fiters passed to this function\n",
        "  params=dict()\n",
        "  params['year']=year\n",
        "\n",
        "  if quarter is not None:\n",
        "    sql += ' AND quarter=:quarter'\n",
        "    params['quarter']=quarter[0]\n",
        "\n",
        "  sql += ' AND geo_id=:geo_id'\n",
        "  if state is not None:\n",
        "    params['geo_id']=state\n",
        "  else:\n",
        "    params['geo_id']=0\n",
        "\n",
        "  sql_device += ' GROUP BY brand'\n",
        "\n",
        "  stmt = sqlalchemy.text(sql,)\n",
        "  df=pd.read_sql_query(stmt, conn,params=params)\n",
        "\n",
        "  # Process the Totals\n",
        "  usr_total=dict()\n",
        "  usr_total['title']='Users'\n",
        "  usr_total['value']=0\n",
        "\n",
        "  app_total=dict()\n",
        "  app_total['title']='Application Opens'\n",
        "  app_total['value']=0\n",
        "\n",
        "  for idx, row in df.iterrows():\n",
        "    usr_total['value'] += row['Users']\n",
        "    app_total['value'] += row['Application Opens']\n",
        "\n",
        "  grand['charts'].append(usr_total)\n",
        "  grand['charts'].append(app_total)\n",
        "\n",
        "  # Process device totals\n",
        "  stmt = sqlalchemy.text(sql_device,)\n",
        "  df_device=pd.read_sql_query(stmt, conn,params=params)\n",
        "\n",
        "  for idx, row in df_device.iterrows():\n",
        "    dev_ct=dict()\n",
        "    dev_ct['title']=row['Device Brand']\n",
        "    dev_ct['value']=row['Device Count']\n",
        "    dev_ct['metric']='Device Count'\n",
        "    det['charts'].append(dev_ct)\n",
        "\n",
        "    dev_pct=dict()\n",
        "    dev_pct['title']=row['Device Brand']\n",
        "    dev_pct['value']=row['Device Share (%)']\n",
        "    dev_pct['metric']='Device Share (%)'\n",
        "    det['charts'].append(dev_pct)\n",
        "\n",
        "  # Execute top SQL\n",
        "  if quarter is not None:\n",
        "    dashboard['top']=dict()\n",
        "    sql_top=f'''SELECT g.name as 'Geo', g.geo_type as 'Geo Type', u.reg_users as 'Users' FROM user_top u JOIN geo g ON (u.geo_id=g.id) WHERE year=:year AND quarter=:quarter AND top_in=:top_in'''\n",
        "\n",
        "    params_top=dict()\n",
        "    params_top['year']=year\n",
        "    params_top['quarter']=quarter[0]\n",
        "\n",
        "    if state is not None:\n",
        "      sql_top += ' AND geo_id IN (SELECT id from geo WHERE parent_id=:parent_id)'\n",
        "      params_top['top_in']='STA'\n",
        "      params_top['parent_id']=state\n",
        "    else:\n",
        "      params_top['top_in']='CON'\n",
        "\n",
        "    stmt = sqlalchemy.text(sql_top,)\n",
        "    df_top=pd.read_sql_query(stmt, conn,params=params_top)\n",
        "\n",
        "    # Populate 'top'\n",
        "    top=dashboard['top']\n",
        "    top['name']= 'Users By Top Geo'\n",
        "    top['charts']=list()\n",
        "\n",
        "    #if state is None:\n",
        "    top['charts'].append(dict())\n",
        "    top['charts'][0]['title']='Top States'\n",
        "    top['charts'][0]['data']=df_top[df_top['Geo Type']=='STA'].reset_index().drop(columns=['index']).sort_values('Users', ascending=False)\n",
        "\n",
        "    top['charts'].append(dict())\n",
        "    top['charts'][1]['title']='Top Districts'\n",
        "    top['charts'][1]['data']=df_top[df_top['Geo Type']=='DIS'].reset_index().drop(columns=['index']).sort_values('Users', ascending=False)\n",
        "\n",
        "    top['charts'].append(dict())\n",
        "    top['charts'][2]['title']='Top Pincodes'\n",
        "    top['charts'][2]['data']=df_top[df_top['Geo Type']=='PIN'].reset_index().drop(columns=['index']).sort_values('Users', ascending=False)\n",
        "\n",
        "  # Map Data\n",
        "  if state is None:\n",
        "    dashboard['by_geo']=dict()\n",
        "    by_geo = dashboard['by_geo']\n",
        "    by_geo['name']='User Info By State'\n",
        "\n",
        "    params_states=dict()\n",
        "    params_states['year']=year\n",
        "    sql_states = f'''SELECT SQ.geo_id AS 'State Code', g.name AS 'State', SQ.users AS 'Users', SQ.apps AS 'App Opens'  FROM (SELECT geo_id, sum(u.reg_users) as users, sum(u.app_opens) as apps FROM user_agg u WHERE year=:year'''\n",
        "    if quarter is not None:\n",
        "      sql_states += ' AND quarter=:quarter'\n",
        "      params_states['quarter']=quarter[0]\n",
        "\n",
        "    sql_states += ''' GROUP BY geo_id)SQ JOIN geo g ON (SQ.geo_id=g.id) WHERE geo_type='STA' '''\n",
        "\n",
        "    stmt = sqlalchemy.text(sql_states,)\n",
        "    df_states=pd.read_sql_query(stmt, conn,params=params_states)\n",
        "    df_states['State']=df_states['State'].apply(convert_to_geojson_state)\n",
        "\n",
        "    by_geo['charts']=list()\n",
        "    by_geo['charts'].append({'title':'User Info By State','data':df_states})\n",
        "\n",
        "  return dashboard\n",
        "\n",
        "\n",
        "def fetch_dash(conn,object,year,quarter=None,state=None):\n",
        "  if object == 'Transaction':\n",
        "    return fetch_txn_dash(conn, year,quarter, state)\n",
        "  elif object=='Insurance':\n",
        "    return fetch_ins_dash(conn, year, quarter,state)\n",
        "  else:\n",
        "    return fetch_user_dash(conn, year, quarter,state)\n"
      ],
      "metadata": {
        "id": "oYgginx8pYR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fetch_user_dash(conn,2022,quarter=(1,'Q3'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "X-aCuGxoqC3X",
        "outputId": "23d23fd9-f992-483d-ae89-1b5995ac666f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'conn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1daa10432ac5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfetch_user_dash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquarter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Q3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'conn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ah6k-4xblf74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Data Loader**"
      ],
      "metadata": {
        "id": "13-krr3iOLCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Load ALL data from JSON files in GDrive\n",
        "'''\n",
        "datadir='/content/drive/MyDrive/Guvi/Assignments/Phonepe/pulse/pulse/data'\n",
        "print(f'''Loading data from {datadir}......20mins expected hold time''')\n",
        "data=load_data(datadir)"
      ],
      "metadata": {
        "id": "ujHnGYhD4v-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GEO INDEXER**"
      ],
      "metadata": {
        "id": "NDo6MwyROWE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Save Geo data to CSV files (States, Dis and Pin)\n",
        "'''\n",
        "geo = index_geo_info(datadir)\n",
        "\n",
        "states = pd.DataFrame.from_dict(geo['states'],orient='index').reset_index().rename(columns={'index':'name', 0:'parent'})\n",
        "districts = pd.DataFrame.from_dict(geo['districts'],orient='index').reset_index().rename(columns={'index':'name', 0:'parent'})\n",
        "pincodes = pd.DataFrame.from_dict(geo['pincodes'],orient='index').reset_index().rename(columns={'index':'name', 0:'parent'})\n",
        "states.name = states.name.astype(str)\n",
        "districts.name = districts.name.astype(str)\n",
        "pincodes.name = pincodes.name.astype(str)\n",
        "\n",
        "states.to_csv('states.csv',index=False, sep=',', encoding='utf-8')\n",
        "districts.to_csv('districts.csv',index=False, sep=',', encoding='utf-8')\n",
        "pincodes.to_csv('pincodes.csv',index=False, sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "UTLQzXfVxzl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CSV Writers**"
      ],
      "metadata": {
        "id": "AA-OTGtw2kbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write loaded Data to CSV files\n",
        "'''\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "datadir='/content/drive/MyDrive/Guvi/Assignments/Phonepe/pulse/pulse/data'\n",
        "#datadir='/content/drive/MyDrive/Learn/guvi/labs/Assignments/PhonePePulse/phonepe/pulse/data'\n",
        "\n",
        "\n",
        "#hover_txns = pd.DataFrame(data['transaction']['map'])\n",
        "#hover_users = pd.DataFrame(data['user']['map'])\n",
        "#hover_ins = pd.DataFrame(data['insurance']['map'])\n",
        "\n",
        "#print(f'''hover_txns - {hover_txns.shape}, hover_users - {hover_users.shape}, hover_ins - {hover_ins.shape}''')\n",
        "#print(f'''{hover_txns.head().to_markdown()}''')\n",
        "#print(f'''{hover_users.head().to_markdown()}''')\n",
        "#print(f'''{hover_ins.head().to_markdown()}''')\n",
        "# Write the dataframes into csv files\n",
        "#hover_txns.to_csv('hover_txns.csv',index=False)\n",
        "#hover_users.to_csv('hover_users.csv',index=False)\n",
        "#hover_ins.to_csv('hover_ins.csv',index=False)\n",
        "\n",
        "\n",
        "agg_txns = pd.DataFrame(data['transaction']['aggregated'])\n",
        "agg_users = pd.DataFrame(data['user']['aggregated'])\n",
        "agg_ins = pd.DataFrame(data['insurance']['aggregated'])\n",
        "top_txns = pd.DataFrame(data['transaction']['top'])\n",
        "top_users = pd.DataFrame(data['user']['top'])\n",
        "top_ins = pd.DataFrame(data['insurance']['top'])\n",
        "print(f'''agg_txns - {agg_txns.shape}, agg_users - {agg_users.shape}, agg_ins - {agg_ins.shape}''')\n",
        "print(f'''top_txns - {top_txns.shape}, top_users - {top_users.shape}, top_ins - {top_ins.shape}''')\n",
        "\n",
        "# Write the dataframes into csv files\n",
        "agg_txns.to_csv('agg_txns.csv',sep=',', encoding='utf-8', index=False)\n",
        "agg_users.to_csv('agg_users.csv',sep=',', encoding='utf-8', index=False)\n",
        "agg_ins.to_csv('agg_ins.csv',sep=',', encoding='utf-8', index=False)\n",
        "top_txns.to_csv('top_txns.csv',sep=',', encoding='utf-8',index=False)\n",
        "top_users.to_csv('top_users.csv',sep=',', encoding='utf-8',index=False)\n",
        "top_ins.to_csv('top_ins.csv',sep=',', encoding='utf-8',index=False)"
      ],
      "metadata": {
        "id": "zGVpRJ27KPqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7d8502-785e-4bb0-d3ba-1435a4d2e094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agg_txns - (4619, 9), agg_users - (7215, 10), agg_ins - (590, 9)\n",
            "top_txns - (17074, 9), top_users - (17075, 7), top_ins - (480, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENVIRONMENT CELLS**"
      ],
      "metadata": {
        "id": "Au7uolcWM6cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Configure Google Cloud Project Context\n",
        "'''\n",
        "project_id=\"<your GCP project id>\"\n",
        "!gcloud config set project {project_id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhhEvLMeQZXs",
        "outputId": "e6a180d4-ffa7-4ec1-fd46-9c380b9ae0cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Aunthenticate to Google Colab\n",
        "'''\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "metadata": {
        "id": "_LtR2GkYQSGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-sql-python-connector #package to connect SQL-Python in Cloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "LqKjTOSpLoTT",
        "outputId": "af9fd49a-fbbd-479d-b376-a8b348ab9c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cloud-sql-python-connector\n",
            "  Downloading cloud_sql_python_connector-1.12.0-py2.py3-none-any.whl.metadata (25 kB)\n",
            "Collecting aiofiles (from cloud-sql-python-connector)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from cloud-sql-python-connector) (3.10.2)\n",
            "Requirement already satisfied: cryptography>=42.0.0 in /usr/local/lib/python3.10/dist-packages (from cloud-sql-python-connector) (42.0.8)\n",
            "Requirement already satisfied: Requests in /usr/local/lib/python3.10/dist-packages (from cloud-sql-python-connector) (2.32.3)\n",
            "Collecting google-auth>=2.28.0 (from cloud-sql-python-connector)\n",
            "  Downloading google_auth-2.33.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=42.0.0->cloud-sql-python-connector) (1.17.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.28.0->cloud-sql-python-connector) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.28.0->cloud-sql-python-connector) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.28.0->cloud-sql-python-connector) (4.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from Requests->cloud-sql-python-connector) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from Requests->cloud-sql-python-connector) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from Requests->cloud-sql-python-connector) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from Requests->cloud-sql-python-connector) (2024.7.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=42.0.0->cloud-sql-python-connector) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.28.0->cloud-sql-python-connector) (0.6.0)\n",
            "Downloading cloud_sql_python_connector-1.12.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.33.0-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: aiofiles, google-auth, cloud-sql-python-connector\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.27.0\n",
            "    Uninstalling google-auth-2.27.0:\n",
            "      Successfully uninstalled google-auth-2.27.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 cloud-sql-python-connector-1.12.0 google-auth-2.33.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "34e20854eee348f79c2b9bd1ca727a63"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymysql #Python pkg that creates API access to SQL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uevOz8A2LsvG",
        "outputId": "265b7cc1-91f8-40ea-858b-3fbb97145744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymysql\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/45.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DB CONNECTION CELL**"
      ],
      "metadata": {
        "id": "CjIkMY3TNDFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Connect to Cloud SQL DB to populate data\n",
        "'''\n",
        "\n",
        "from pp_dbconnect import PPDbConnector\n",
        "\n",
        "conname='<your db connection string>'\n",
        "user= '<your user login>'\n",
        "password='<your password>'\n",
        "db='phonepe-bn'\n",
        "pp_connector = PPDbConnector(conname,user,password,db)\n",
        "conn = pp_connector.connect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUq8Fx0wJcZn",
        "outputId": "91def83f-f613-48b5-87ba-f55d89ba41af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to 'phonepe-bn' database!! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Dashboards Cell**"
      ],
      "metadata": {
        "id": "Txq2jiuENJw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_user_dash(conn,2018,quarter=(1,'Q1'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhpNk4iLKtd1",
        "outputId": "6d7fa128-e5b5-46ea-a9cb-59c2b102ab9d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'object': 'User',\n",
              " 'title': 'User Dashboard',\n",
              " 'description': 'Details of User and Devices for the selected timeline',\n",
              " 'grand_total': {'name': 'Registered Users',\n",
              "  'charts': [{'title': 'Users', 'value': 46877867},\n",
              "   {'title': 'Application Opens', 'value': 0}]},\n",
              " 'det_total': {'name': 'Registered Devices',\n",
              "  'charts': [{'title': 'Apple', 'value': 20334366.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Apple', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Huawei', 'value': 9544952.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Huawei', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Lenovo', 'value': 13785649.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Lenovo', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Motorola', 'value': 18622946.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Motorola', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'OnePlus', 'value': 15834429.0, 'metric': 'Device Count'},\n",
              "   {'title': 'OnePlus', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Oppo', 'value': 59167154.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Oppo', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Others', 'value': 64170504.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Others', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Realme', 'value': 29005731.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Realme', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Samsung', 'value': 119041931.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Samsung', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Vivo', 'value': 80690035.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Vivo', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Xiaomi', 'value': 151823416.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Xiaomi', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Gionee', 'value': 723312.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Gionee', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Asus', 'value': 430392.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Asus', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Micromax', 'value': 2410518.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Micromax', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'COOLPAD', 'value': 10.0, 'metric': 'Device Count'},\n",
              "   {'title': 'COOLPAD', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Lyf', 'value': 1271.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Lyf', 'value': 0.0, 'metric': 'Device Share (%)'},\n",
              "   {'title': 'Lava', 'value': 10441.0, 'metric': 'Device Count'},\n",
              "   {'title': 'Lava', 'value': 0.0, 'metric': 'Device Share (%)'}]},\n",
              " 'top': {'name': 'Users By Top Geo',\n",
              "  'charts': [{'title': 'Top States',\n",
              "    'data':               Geo Geo Type    Users\n",
              "    5     maharashtra      STA  6106994\n",
              "    8   uttar-pradesh      STA  4694250\n",
              "    3       karnataka      STA  3717763\n",
              "    0  andhra-pradesh      STA  3336450\n",
              "    7       telangana      STA  3315560\n",
              "    6       rajasthan      STA  3158202\n",
              "    2         gujarat      STA  2690048\n",
              "    9     west-bengal      STA  2604789\n",
              "    4  madhya-pradesh      STA  2553603\n",
              "    1           bihar      STA  2133804},\n",
              "   {'title': 'Top Districts',\n",
              "    'data':                   Geo Geo Type    Users\n",
              "    2     bengaluru urban      DIS  1922368\n",
              "    3                pune      DIS  1211643\n",
              "    6              jaipur      DIS   900773\n",
              "    5     mumbai suburban      DIS   719300\n",
              "    7           hyderabad      DIS   655175\n",
              "    8          rangareddy      DIS   635175\n",
              "    4               thane      DIS   631864\n",
              "    1           ahmadabad      DIS   572811\n",
              "    9  medchal malkajgiri      DIS   549429\n",
              "    0       visakhapatnam      DIS   497187},\n",
              "   {'title': 'Top Pincodes',\n",
              "    'data':       Geo Geo Type   Users\n",
              "    9  201301      PIN  114625\n",
              "    8  500072      PIN  105012\n",
              "    5  560068      PIN   98487\n",
              "    1  110059      PIN   95496\n",
              "    2  110092      PIN   83600\n",
              "    4  122001      PIN   82656\n",
              "    6  560037      PIN   75304\n",
              "    3  395006      PIN   72275\n",
              "    0  800001      PIN   71092\n",
              "    7  302012      PIN   68098}]},\n",
              " 'by_geo': {'name': 'User Info By State',\n",
              "  'charts': [{'title': 'User Info By State',\n",
              "    'data':     State Code                               State      Users  App Opens\n",
              "    0            1           Andaman & Nicobar Islands     6740.0        0.0\n",
              "    1            2                      Andhra Pradesh  3336450.0        0.0\n",
              "    2            3                   Arunachal Pradesh    21495.0        0.0\n",
              "    3            4                               Assam   385237.0        0.0\n",
              "    4            5                               Bihar  2133804.0        0.0\n",
              "    5            6                          Chandigarh    86268.0        0.0\n",
              "    6            7                        Chhattisgarh   679263.0        0.0\n",
              "    7            8  Dadra & Nagar Haveli & Daman & Diu    53258.0        0.0\n",
              "    8            9                               Delhi  2087022.0        0.0\n",
              "    9           10                                 Goa    91936.0        0.0\n",
              "    10          11                             Gujarat  2690048.0        0.0\n",
              "    11          12                             Haryana  1847357.0        0.0\n",
              "    12          13                    Himachal Pradesh   227373.0        0.0\n",
              "    13          14                     Jammu & Kashmir   176425.0        0.0\n",
              "    14          15                           Jharkhand   885762.0        0.0\n",
              "    15          16                           Karnataka  3717763.0        0.0\n",
              "    16          17                              Kerala   784184.0        0.0\n",
              "    17          18                              Ladakh     8784.0        0.0\n",
              "    18          19                         Lakshadweep      501.0        0.0\n",
              "    19          20                      Madhya Pradesh  2553603.0        0.0\n",
              "    20          21                         Maharashtra  6106994.0        0.0\n",
              "    21          22                             Manipur    29925.0        0.0\n",
              "    22          23                           Meghalaya    18513.0        0.0\n",
              "    23          24                             Mizoram     7593.0        0.0\n",
              "    24          25                            Nagaland    15074.0        0.0\n",
              "    25          26                              Odisha  1594015.0        0.0\n",
              "    26          27                          Puducherry    49318.0        0.0\n",
              "    27          28                              Punjab   847142.0        0.0\n",
              "    28          29                           Rajasthan  3158202.0        0.0\n",
              "    29          30                              Sikkim    20051.0        0.0\n",
              "    30          31                          Tamil Nadu  2104754.0        0.0\n",
              "    31          32                           Telangana  3315560.0        0.0\n",
              "    32          33                             Tripura    55157.0        0.0\n",
              "    33          34                       Uttar Pradesh  4694250.0        0.0\n",
              "    34          35                         Uttarakhand   483043.0        0.0\n",
              "    35          36                         West Bengal  2604789.0        0.0}]}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Save Geo Data into DB\n",
        "'''\n",
        "# Connect to Cloud SQL DB to populate data\n",
        "import traceback\n",
        "from pp_dbconnect import PPDbConnector\n",
        "\n",
        "conname='<your db connection string>'\n",
        "user= '<your sql login>'\n",
        "password='<your sql password>'\n",
        "db='phonepe-bn'\n",
        "pp_connector = PPDbConnector(conname,user,password,db)\n",
        "conn = pp_connector.connect()\n",
        "import pandas as pd\n",
        "states = pd.read_csv('states.csv')\n",
        "districts = pd.read_csv('districts.csv')\n",
        "pincodes = pd.read_csv('pincodes.csv')\n",
        "\n",
        "print(f'''states - {states.shape}, districts - {districts.shape}, pincodes - {pincodes.shape}''')\n",
        "\n",
        "try:\n",
        "  save_geo(states,districts,pincodes,conn)\n",
        "  print('Saved Geo information in DB successfully !!')\n",
        "except:\n",
        "  print('''Failed to save 'geo' information - Ensure 'geo' table is empty before running this !!''')\n",
        "  print(traceback.print_exc())"
      ],
      "metadata": {
        "id": "_QtLmJGiS2Wy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb799781-bc17-4a29-e8f5-903061281f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to 'phonepe-bn' database!! \n",
            "states - (36, 2), districts - (417, 2), pincodes - (1115, 2)\n",
            "Saved Geo information in DB successfully !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Populate Transaction Data into DB\n",
        "'''\n",
        "import pandas as pd\n",
        "import traceback\n",
        "agg_txns=pd.read_csv('agg_txns.csv')\n",
        "top_txns=pd.read_csv('top_txns.csv')\n",
        "\n",
        "print(f'''agg_txns - {agg_txns.shape}, top_txns - {top_txns.shape}''')\n",
        "\n",
        "try:\n",
        "  save_txns(agg_txns,top_txns,conn)\n",
        "  print('Saved Transaction information in DB successfully !!')\n",
        "except:\n",
        "  print('''Failed to save 'transaction' information - Ensure 'transaction' table is empty before running this !!''')\n",
        "  print(traceback.print_exc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZxlMGEn8-bN",
        "outputId": "852af14c-18cb-4e6e-bccb-bd3006e94200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agg_txns - (4619, 9), top_txns - (17074, 9)\n",
            "geo - (1569, 4)\n",
            "geo_type = PIN, geo_parent = ladakh\n",
            "geo_type = PIN, geo_parent = ladakh\n",
            "agg_txns(db): (4619, 8), top_txns(db):(17074, 8)\n",
            "Saved Transaction information in DB successfully !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Populate Insurance Data into DB\n",
        "'''\n",
        "import pandas as pd\n",
        "import traceback\n",
        "agg_ins=pd.read_csv('agg_ins.csv')\n",
        "top_ins=pd.read_csv('top_ins.csv')\n",
        "\n",
        "print(f'''agg_ins - {agg_ins.shape}, top_ins - {top_ins.shape}''')\n",
        "\n",
        "try:\n",
        "  save_ins(agg_ins,top_ins,conn)\n",
        "  print('Saved Insurance information in DB successfully !!')\n",
        "except:\n",
        "  print('''Failed to save 'insurance' information - Ensure 'insurance' table is empty before running this !!''')\n",
        "  print(traceback.print_exc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQRbC_SWMmvH",
        "outputId": "ef4f928a-dfde-46ee-d3d4-8bda77365b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agg_ins - (590, 9), top_ins - (480, 9)\n",
            "geo - (1569, 4)\n",
            "agg_ins(db): (590, 8), top_ins(db):(480, 8)\n",
            "Saved Insurance information in DB successfully !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Populate USERS Data into DB\n",
        "'''\n",
        "import pandas as pd\n",
        "import traceback\n",
        "agg_users=pd.read_csv('agg_users.csv')\n",
        "top_users=pd.read_csv('top_users.csv')\n",
        "\n",
        "print(f'''agg_users - {agg_users.shape}, top_users - {top_users.shape}''')\n",
        "\n",
        "try:\n",
        "  save_users(agg_users,top_users,conn)\n",
        "  print('Saved USERS information in DB successfully !!')\n",
        "except:\n",
        "  print('''Failed to save 'users' information - Ensure 'users' tables are empty before running this !!''')\n",
        "  print(traceback.print_exc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeX3zQxhOWK_",
        "outputId": "c734a98e-c9fc-4063-9967-31fe8aa99c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agg_users - (7215, 10), top_users - (17075, 7)\n",
            "geo - (1569, 4)\n",
            "agg_users(db): (7215, 9), top_users(db):(17075, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-81-ff65c745f81a>:458: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  u_agg.drop_duplicates(inplace=True,ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_agg(db): (925, 7), device_agg(db): (7215, 7), top_users(db):(17075, 7)\n",
            "Saved USERS information in DB successfully !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_txns[(top_txns.year==2018) & (top_txns.quarter==1) & (top_txns.geo_id==102)].reset_index()"
      ],
      "metadata": {
        "id": "NLYC088CcvyQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "a733b499-197e-469c-9887-d62fe28c0555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index  year  quarter top_in stat_type   count        amount  geo_id\n",
              "0   3353  2018        1    STA     TOTAL  158509  1.698650e+08     102"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8fb707c-e149-49de-8b7f-2b9486c0889b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>year</th>\n",
              "      <th>quarter</th>\n",
              "      <th>top_in</th>\n",
              "      <th>stat_type</th>\n",
              "      <th>count</th>\n",
              "      <th>amount</th>\n",
              "      <th>geo_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3353</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>STA</td>\n",
              "      <td>TOTAL</td>\n",
              "      <td>158509</td>\n",
              "      <td>1.698650e+08</td>\n",
              "      <td>102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8fb707c-e149-49de-8b7f-2b9486c0889b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b8fb707c-e149-49de-8b7f-2b9486c0889b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b8fb707c-e149-49de-8b7f-2b9486c0889b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"top_txns[(top_txns\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 3353,\n        \"max\": 3353,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3353\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 2018,\n        \"max\": 2018,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2018\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quarter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top_in\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"STA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stat_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"TOTAL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 158509,\n        \"max\": 158509,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          158509\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"amount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 169865045.945706,\n        \"max\": 169865045.945706,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          169865045.945706\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"geo_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 102,\n        \"max\": 102,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          102\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h4kjOM4pC1Yr"
      }
    }
  ]
}